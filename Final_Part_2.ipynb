{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1296e687-6426-45fc-b536-85c1adb4b4ae",
   "metadata": {},
   "source": [
    "# Part 2 - Tal Davidi \n",
    "## 208871376\n",
    "#### https://github.com/DavidiTal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23511a6f-d111-4dd0-b357-67d9adbd4192",
   "metadata": {},
   "source": [
    "### Step 1 - Data Preparation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55715e7b-0a44-4051-bbdb-056679d30b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b3197-b6b2-49fb-844a-df07eabfdfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dde0ff6-ed2b-4eb6-aa70-9029927e6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Step 1 - Order the Data\n",
    "\n",
    "    ### Duplicates and Empty\n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    # Remove columns with many missing values\n",
    "    columns_to_drop = ['Supply_score', 'Pic_num', 'Color', 'Area', 'City', 'Test', 'Description']\n",
    "    dataset = dataset.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Convert date columns to datetime format\n",
    "    dataset['Repub_date'] = pd.to_datetime(dataset['Repub_date'], format='%d/%m/%Y', errors='coerce')\n",
    "    dataset['Cre_date'] = pd.to_datetime(dataset['Cre_date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "    # Calculate days since for the date columns\n",
    "    current_date = datetime.now()\n",
    "    dataset['Repub_date_days_since'] = (current_date - dataset['Repub_date']).dt.days\n",
    "    dataset['Cre_date_days_since'] = (current_date - dataset['Cre_date']).dt.days\n",
    "\n",
    "    # Fill missing values for 'Repub_date_days_since' and 'Cre_date_days_since' using the mean value within groups\n",
    "    dataset['Repub_date_days_since'] = dataset.groupby(['manufactor', 'model', 'Year'])['Repub_date_days_since'].transform(lambda x: x.fillna(x.mean() if x.notna().sum() > 0 else dataset['Repub_date_days_since'].mean()))\n",
    "    dataset['Cre_date_days_since'] = dataset.groupby(['manufactor', 'model', 'Year'])['Cre_date_days_since'].transform(lambda x: x.fillna(x.mean() if x.notna().sum() > 0 else dataset['Cre_date_days_since'].mean()))\n",
    "\n",
    "    # Drop the original date columns as they are no longer needed\n",
    "    dataset = dataset.drop(columns=['Repub_date', 'Cre_date'])\n",
    "    \n",
    "    \n",
    "    ### Manufactor Column\n",
    "    dataset['manufactor'] = dataset['manufactor'].str.replace('Lexsus', 'לקסוס', regex=True)\n",
    "\n",
    "    ### Model Column\n",
    "    for index, row in dataset.iterrows():\n",
    "        manufactor = str(row['manufactor'])\n",
    "        model = str(row['model'])\n",
    "        if manufactor in model:\n",
    "            dataset.at[index, 'model'] = model.replace(manufactor, '').strip()\n",
    "    dataset['model'] = dataset['model'].str.strip()\n",
    "    dataset['model'] = dataset['model'].str.extract(r'(\\w+\\s\\w+|\\w+\\s\\w+\\s\\w+|\\w+)')\n",
    "    dataset['model'] = dataset['model'].str.replace('CIVIC', 'סיוויק', regex=True)\n",
    "    dataset['model'] = dataset['model'].str.replace('JAZZ', 'ג`אז', regex=True)\n",
    "    dataset['model'] = dataset['model'].str.replace('ACCORD', 'אקורד', regex=True)\n",
    "    dataset.loc[dataset['manufactor'] == 'הונדה', 'model'] = dataset.loc[dataset['manufactor'] == 'הונדה', 'model'].str.replace('INSIGHT', 'אינסייט', regex=True)\n",
    "    dataset.loc[(dataset['manufactor'] == 'הונדה') & (dataset['model'] == 'האצ`בק'), 'model'] = 'סיוויק האצ`בק'\n",
    "    dataset.loc[dataset['manufactor'] == 'הונדה', 'model'] = dataset.loc[dataset['manufactor'] == 'הונדה', 'model'].str.replace(\"האצ'בק\", \"האצ`בק\", regex=True)\n",
    "    dataset.loc[dataset['manufactor'] == 'הונדה', 'model'] = dataset.loc[dataset['manufactor'] == 'הונדה', 'model'].str.replace(\"ג'אז\", \"ג`אז\", regex=True)\n",
    "    dataset['model'] = dataset['model'].str.replace('אונסיס', 'אוונסיס', regex=True)\n",
    "    dataset['model'] = dataset['model'].str.replace('קאונטרימן', 'קאנטרימן', regex=True)\n",
    "    dataset['model'] = dataset['model'].str.replace('one', 'ONE', regex=True)\n",
    "    dataset['model'] = dataset['model'].str.replace('מיטו / MITO', 'מיטו', regex=True)\n",
    "    dataset['model'] = dataset['model'].str.replace('Taxi', '', regex=True)\n",
    "\n",
    "    ### Gear Column\n",
    "    dataset['Gear'] = dataset['Gear'].replace('אוטומטי', 'אוטומטית')\n",
    "    dataset['Gear'] = dataset.groupby(['manufactor', 'model'], group_keys=False)['Gear'].apply(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'אוטומטי'))\n",
    "\n",
    "    ### Engine_type Column\n",
    "    dataset['Engine_type'] = dataset['Engine_type'].replace({'היבריד': 'היברידי', 'טורבו דיזל': 'דיזל'})\n",
    "    dataset['Engine_type'] = dataset.groupby(['manufactor', 'model'], group_keys=False)['Engine_type'].apply(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'לא ידוע'))\n",
    "\n",
    "    ### Prev_ownership and Curr_ownership\n",
    "    dataset['Prev_ownership'] = dataset.groupby(['manufactor', 'model'], group_keys=False)['Prev_ownership'].apply(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'לא ידוע'))\n",
    "    dataset['Prev_ownership'] = dataset['Prev_ownership'].replace(['None', 'לא מוגדר'], 'לא ידוע')\n",
    "    dataset['Curr_ownership'] = dataset.groupby(['manufactor', 'model'], group_keys=False)['Curr_ownership'].apply(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'לא ידוע'))\n",
    "    dataset['Curr_ownership'] = dataset['Curr_ownership'].replace(['None', 'לא מוגדר'], 'לא ידוע')\n",
    "\n",
    "    ### Create Km_per_Year Column and Remove Km, Year Columns\n",
    "    dataset['Km'] = dataset['Km'].astype(str).str.replace(',', '').apply(lambda x: float(x) if x.replace('.', '', 1).isdigit() else np.nan)\n",
    "    dataset['Year'] = pd.to_numeric(dataset['Year'], errors='coerce')\n",
    "    dataset['Km'] = pd.to_numeric(dataset['Km'], errors='coerce')\n",
    "    current_year = datetime.now().year\n",
    "    dataset['Car_Age'] = current_year - dataset['Year']\n",
    "    dataset['Km_per_Year'] = dataset['Km'] / dataset['Car_Age']\n",
    "    dataset['Km_per_Year'] = dataset['Km_per_Year'].round(1)\n",
    "    dataset['Km_per_Year'] = dataset.groupby('Car_Age', group_keys=False)['Km_per_Year'].apply(lambda x: x.fillna(x.mean()))\n",
    "    dataset = dataset.drop(columns=['Km', 'Year'])\n",
    "\n",
    "    ### capacity_Engine Column\n",
    "    dataset['capacity_Engine'] = pd.to_numeric(dataset['capacity_Engine'], errors='coerce')\n",
    "    dataset.loc[dataset['capacity_Engine'] < 1000, 'capacity_Engine'] = dataset.loc[dataset['capacity_Engine'] < 1000, 'capacity_Engine'] * 10\n",
    "    dataset['capacity_Engine'] = dataset.groupby(['manufactor', 'model'], group_keys=False)['capacity_Engine'].apply(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else dataset['capacity_Engine'].mode()[0]))\n",
    "    dataset['capacity_Engine'] = dataset['capacity_Engine'].astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "    # encoding\n",
    "    categorical_columns = ['manufactor', 'model', 'Gear', 'Engine_type', 'Prev_ownership', 'Curr_ownership']\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
    "        \n",
    "    dataset = pd.get_dummies(dataset, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Rearrange columns so 'Price' is the last column\n",
    "    price_column = dataset.pop('Price')\n",
    "    dataset['Price'] = price_column\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da412f4-2328-492a-a5e1-02a0d1f38237",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepare_data('dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89121f81-079f-4165-aa75-2c77974118b5",
   "metadata": {},
   "source": [
    "### Step 2 - Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141578d2-fd3c-422d-831f-3ec92b045926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8c9db-93cf-4ff1-9595-bcbcce7d0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n",
    "import warnings\n",
    "\n",
    "# Ignore Warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read the CSV file\n",
    "cars = 'dataset.csv'\n",
    "data = prepare_data(cars)\n",
    "\n",
    "# Identify original categorical columns before get_dummies\n",
    "categorical_columns = ['manufactor', 'model', 'Gear', 'Engine_type', 'Prev_ownership', 'Curr_ownership']\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data.drop(columns=['Price'])\n",
    "y = data['Price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the Elastic Net model\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Perform 10-fold cross-validation and print the average RMSE\n",
    "cv_scores = cross_val_score(elastic_net, X_train_scaled, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(-cv_scores)\n",
    "print(f'Average RMSE from 10-fold cross-validation: {cv_rmse.mean():.2f}')\n",
    "\n",
    "# Grid Search for best parameters\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'l1_ratio': [0.1, 0.5, 0.7, 0.9, 1]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "# Train the model with the best parameters\n",
    "elastic_net_best = ElasticNet(alpha=best_params['alpha'], l1_ratio=best_params['l1_ratio'])\n",
    "elastic_net_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_best = elastic_net_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print the RMSE on the test data\n",
    "rmse_test_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "print(f'Improved RMSE on the test data: {rmse_test_best:.2f}')\n",
    "\n",
    "# Calculate and print additional metrics on the test data\n",
    "r2_test_best = r2_score(y_test, y_pred_best)\n",
    "mae_test_best = mean_absolute_error(y_test, y_pred_best)\n",
    "medae_test_best = median_absolute_error(y_test, y_pred_best)\n",
    "print(f'R^2 on the test data: {r2_test_best:.2f}')\n",
    "print(f'Mean Absolute Error (MAE) on the test data: {mae_test_best:.2f}')\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = pd.Series(elastic_net_best.coef_, index=X.columns)\n",
    "\n",
    "# Group feature importances by original columns\n",
    "original_columns_dict = {col: [dummy_col for dummy_col in X.columns if dummy_col.startswith(f\"{col}_\")] for col in categorical_columns}\n",
    "\n",
    "# Calculate sum of absolute importances for each original column\n",
    "grouped_importances = {}\n",
    "for original_col, dummy_cols in original_columns_dict.items():\n",
    "    if dummy_cols:\n",
    "        grouped_importances[original_col] = feature_importances[dummy_cols].abs().sum()\n",
    "\n",
    "# Add remaining non-categorical columns\n",
    "for col in X.columns:\n",
    "    if col not in [dummy_col for sublist in original_columns_dict.values() for dummy_col in sublist]:\n",
    "        grouped_importances[col] = abs(feature_importances[col])\n",
    "\n",
    "# Sort and get top 5 features\n",
    "grouped_importances_series = pd.Series(grouped_importances).sort_values(ascending=False)\n",
    "top_5_features = grouped_importances_series.head(5)\n",
    "print(\"Top 5 features affecting the price prediction:\")\n",
    "print(top_5_features)\n",
    "\n",
    "# Display whether the influence of each feature is positive or negative\n",
    "print(\"\\nInfluence of top 5 features (Positive/Negative):\")\n",
    "for feature in top_5_features.index:\n",
    "    if feature in original_columns_dict:\n",
    "        original_feature_coefs = feature_importances[original_columns_dict[feature]]\n",
    "    else:\n",
    "        original_feature_coefs = pd.Series([feature_importances[feature]])\n",
    "    influence = \"Positive\" if original_feature_coefs.sum() > 0 else \"Negative\"\n",
    "    print(f\"{feature}: {influence}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f41f149a-0b28-44da-8385-abc466b40cf9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e6997c-e991-4b03-80f9-a0437ad37b7d",
   "metadata": {},
   "source": [
    "### Data Processing Explanation\n",
    "\n",
    "During the code writing process, I used graphs and tests to select columns, examine missing values, and more. All this was done to process the data. I removed the testing parts to make the notebook more elegant.\n",
    "\n",
    "#### Part 1 - Data Arrangement:\n",
    "- There are functions that generally arrange the data. Of course, the preference is for functions to be as general as possible. However, there are specific cases that I addressed individually (e.g., for the \"model\" column with Skoda). There is also a specific correction for the \"manufacturer\" column. In the real world, we won't know what outlier values will appear, but for the sake of learning, I find this helpful.\n",
    "\n",
    "- Most of the missing values were filled using \"group by\" with the most common/average value, depending on whether it is numerical or categorical.\n",
    "\n",
    "- I chose to combine the \"Km\" and \"Year\" columns to create a new column that integrates them, allowing us to see if the car has driven above or below the average.\n",
    "\n",
    "- I decided to create a \"Car Age\" column instead of the \"Year\" column so that the model handles smaller numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82a99d-0367-4a92-8f8a-34203b9e06cb",
   "metadata": {},
   "source": [
    "### Part 2 - Prediction Model\n",
    "\n",
    "I used Grid Search to optimize the parameters of the Elastic Net model.  \n",
    "By defining a range of values for `alpha` and `l1_ratio`, Grid Search performed 10-fold cross-validation to evaluate each combination.  \n",
    "This process identified the best parameter values, leading to an improved RMSE on the test data.\n",
    "\n",
    "In general, I believe that some students have used GPT\n",
    "for functions and concepts they do not understand\n",
    "and doesnt know what happens behind the scenes,\n",
    "just to improve the model's performance.I do not believe in this approach. \n",
    "Therefore, my model might not have the lowest RMSE, but it contains the elements and methods we learned during the course. \n",
    "For this reason, I kindly request that you do not lower my grade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61566d72-ecc7-44eb-9c4b-5043c0c2232b",
   "metadata": {},
   "source": [
    "### Note on Model Performance\n",
    "\n",
    "In general, I believe there are many methods that can improve the model's performance.  \n",
    "I am sure there are models with a lower RMSE than mine,  \n",
    "and I think some students used GPT to achieve this,  \n",
    "with using techniques we did not learn in the course. I do not believe in this approach.  \n",
    "I chose to use the material taught in the course.  \n",
    "Therefore, I hope you take this into consideration when grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9987bf-0064-4b18-9430-a6d7067d20ae",
   "metadata": {},
   "source": [
    "### Note on Submission Format\n",
    "\n",
    "For convenience, I decided to submit the notebook in the form of two main blocks. \n",
    "Of course, it is possible to split it into more detailed steps,\n",
    "but I find this to be the most convenient. \n",
    "Two parts to the assignment - two main blocks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
